eval_interval: 200
eval_iters: 200
log_interval: 10
always_save_checkpoint: false
gradient_accumulation_steps: 10

# data
dataset: 'poems'
checkpoint_name: 'poems_from_poe'
batch_size: 4
block_size: 256

# model
n_layer: 12
n_head: 12
n_embd: 384
dropout: 0.0
bias: False
vocab_size: 50304

learning_rate: 0.00001
max_iters: 2001
lr_decay_iters: 2001
min_lr: 0.0000001
warmup_iters: 100